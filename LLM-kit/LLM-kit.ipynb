{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9477753",
   "metadata": {},
   "source": [
    "# CRNN-based Voice Activity Detection (VAD) — Training Notebook\n",
    "\n",
    "**EN**: This notebook mirrors `vad_train_crnn.py` with clear, sectioned explanations and bilingual comments.  \n",
    "**KO**: 이 노트북은 `vad_train_crnn.py`의 내용을 섹션별 설명과 한/영 주석으로 정리한 버전입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844f64b",
   "metadata": {},
   "source": [
    "## 0. Environment / Imports\n",
    "\n",
    "**EN**: Core libraries for audio I/O, feature extraction, PyTorch model training, and utilities.  \n",
    "**KO**: 오디오 입출력, 특징 추출, PyTorch 학습 및 각종 유틸리티 임포트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, json, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "print(f'[info] torch={torch.__version__} cuda={torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d7177",
   "metadata": {},
   "source": [
    "## 1. Reproducibility Utility\n",
    "\n",
    "**EN**: Sets seeds and toggles CuDNN flags for stable, fast training.  \n",
    "**KO**: 시드 고정 및 CuDNN 설정으로 재현성과 속도를 확보합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 1337):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False  # allow TF32\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df92f7e",
   "metadata": {},
   "source": [
    "## 2. Audio Utilities\n",
    "\n",
    "**EN**: Load mono audio and compute log-mel spectrograms.  \n",
    "**KO**: 모노 오디오 로드 및 로그-멜 스펙트로그램 계산 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff38f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path: Path, target_sr: int) -> np.ndarray:\n",
    "    wav, sr = sf.read(str(path), dtype=\"float32\", always_2d=False)\n",
    "    if wav.ndim > 1:\n",
    "        wav = np.mean(wav, axis=1)\n",
    "    if sr != target_sr:\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "    return wav\n",
    "\n",
    "def to_logmel(\n",
    "    wav: np.ndarray,\n",
    "    sr: int,\n",
    "    n_fft: int = 1024,\n",
    "    hop_length: int = 160,\n",
    "    win_length: int = 400,\n",
    "    n_mels: int = 64,\n",
    "    fmin: int = 50,\n",
    "    fmax: Optional[int] = None,\n",
    "    eps: float = 1e-10,\n",
    ") -> np.ndarray:\n",
    "    spec = np.abs(librosa.stft(wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length)) ** 2\n",
    "    mel = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax or sr//2)\n",
    "    mel_spec = np.dot(mel, spec)\n",
    "    logmel = np.log(mel_spec + eps)\n",
    "    return logmel.T  # (time, n_mels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918724e4",
   "metadata": {},
   "source": [
    "## 3. Dataset: Frame-level Labels\n",
    "\n",
    "**EN**: Reads pairs of `(wav_path, label_npy_path)` and returns `(T,F)` log-mel features with frame-level labels `(T,)`.  \n",
    "**KO**: `(오디오, 라벨)` 쌍을 읽고 `(T,F)` 로그멜과 `(T,)` 프레임 단위 라벨을 반환합니다.\n",
    "\n",
    "> Augmentation: Optional noise mixing using files from `noise_dir` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameLabelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        items: List[Tuple[Path, Path]],\n",
    "        sr: int = 16000,\n",
    "        n_fft: int = 1024,\n",
    "        hop_length: int = 160,\n",
    "        win_length: int = 400,\n",
    "        n_mels: int = 64,\n",
    "        augment_noise_paths: Optional[List[Path]] = None,\n",
    "        snr_db_range: Tuple[float, float] = (5.0, 20.0),\n",
    "    ):\n",
    "        self.items = items\n",
    "        self.sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.n_mels = n_mels\n",
    "        self.augment_noise_paths = augment_noise_paths or []\n",
    "        self.snr_db_range = snr_db_range\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def _mix_noise(self, clean: np.ndarray) -> np.ndarray:\n",
    "        if not self.augment_noise_paths:\n",
    "            return clean\n",
    "        noise_path = random.choice(self.augment_noise_paths)\n",
    "        noise = load_audio(noise_path, self.sr)\n",
    "        if len(noise) < len(clean):\n",
    "            reps = math.ceil(len(clean) / len(noise))\n",
    "            noise = np.tile(noise, reps)[: len(clean)]\n",
    "        else:\n",
    "            start = random.randint(0, len(noise) - len(clean))\n",
    "            noise = noise[start : start + len(clean)]\n",
    "        snr_db = random.uniform(*self.snr_db_range)\n",
    "        sig_pwr = np.mean(clean**2) + 1e-12\n",
    "        noise_pwr = np.mean(noise**2) + 1e-12\n",
    "        scale = np.sqrt(sig_pwr / (10 ** (snr_db / 10) * noise_pwr))\n",
    "        return clean + scale * noise\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, label_path = self.items[idx]\n",
    "        wav = load_audio(wav_path, self.sr)\n",
    "        if random.random() < 0.7:\n",
    "            wav = self._mix_noise(wav)\n",
    "\n",
    "        logmel = to_logmel(\n",
    "            wav, sr=self.sr,\n",
    "            n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length,\n",
    "            n_mels=self.n_mels\n",
    "        )  # (T, F)\n",
    "\n",
    "        labels = np.load(label_path)  # shape (T,) with {0,1}\n",
    "        T = min(len(labels), logmel.shape[0])\n",
    "        logmel = logmel[:T]\n",
    "        labels = labels[:T]\n",
    "\n",
    "        x = torch.tensor(logmel, dtype=torch.float32)  # (T, F)\n",
    "        y = torch.tensor(labels, dtype=torch.float32)  # (T,)\n",
    "        return x, y\n",
    "\n",
    "def collate_pad(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = [x.shape[0] for x in xs]\n",
    "    max_len = max(lengths)\n",
    "    Fdim = xs[0].shape[1]\n",
    "    x_pad = torch.zeros(len(xs), max_len, Fdim, dtype=torch.float32)\n",
    "    y_pad = torch.zeros(len(xs), max_len, dtype=torch.float32)\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        T = x.shape[0]\n",
    "        x_pad[i, :T] = x\n",
    "        y_pad[i, :T] = y\n",
    "    return x_pad, y_pad, torch.tensor(lengths, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f09ae",
   "metadata": {},
   "source": [
    "## 4. Model: CRNN for VAD\n",
    "\n",
    "**EN**: Lightweight CNN over frequency + GRU over time → frame-level logits.  \n",
    "**KO**: 주파수 축 CNN, 시간 축 GRU로 구성된 경량 CRNN → 프레임 로짓 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNNVAD(nn.Module):\n",
    "    def __init__(self, n_mels: int = 64, cnn_channels: int = 64, rnn_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, cnn_channels, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(cnn_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.rnn = nn.GRU(input_size=cnn_channels * n_mels, hidden_size=rnn_hidden,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(2 * rnn_hidden, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\n",
    "        # x: (B, T, F)\n",
    "        B, T, Fdim = x.shape\n",
    "        x = x.unsqueeze(1)                 # (B, 1, T, F)\n",
    "        x = self.conv(x)                   # (B, C, T, F)\n",
    "        B, C, T, Fdim = x.shape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(B, T, C * Fdim)  # (B, T, C*F)\n",
    "\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed)\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T, 2H)\n",
    "        logits = self.out(out).squeeze(-1)  # (B, T)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f59a07",
   "metadata": {},
   "source": [
    "## 5. Loss & Metrics\n",
    "\n",
    "**EN**: BCE loss with padding mask; frame-level F1 metric for validation.  \n",
    "**KO**: 패딩을 무시하는 BCE 손실과 프레임 F1 평가지표."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_with_mask(logits: torch.Tensor, targets: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "    B, T = logits.shape\n",
    "    mask = torch.arange(T, device=logits.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits[mask], targets[mask])\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def frame_f1(logits: torch.Tensor, targets: torch.Tensor, lengths: torch.Tensor, thresh: float = 0.5) -> float:\n",
    "    B, T = logits.shape\n",
    "    mask = torch.arange(T, device=logits.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    preds = (torch.sigmoid(logits) > thresh) & mask\n",
    "    targs = (targets > 0.5) & mask\n",
    "    tp = (preds & targs).sum().item()\n",
    "    fp = (preds & ~targs).sum().item()\n",
    "    fn = (~preds & targs).sum().item()\n",
    "    precision = tp / (tp + fp + 1e-9)\n",
    "    recall = tp / (tp + fn + 1e-9)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    return float(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f587ba",
   "metadata": {},
   "source": [
    "## 6. Training / Evaluation Loops\n",
    "\n",
    "**EN**: AMP-enabled training and evaluation with average F1.  \n",
    "**KO**: AMP 지원 학습 루프와 평균 F1 평가 루프입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    for x, y, lengths in loader:\n",
    "        x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=device.type == \"cuda\"):\n",
    "            logits = model(x, lengths)\n",
    "            loss = bce_with_mask(logits, y, lengths)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "    return total_loss / max(total_batches, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_f1 = 0.0\n",
    "    total_batches = 0\n",
    "    for x, y, lengths in loader:\n",
    "        x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "        logits = model(x, lengths)\n",
    "        total_f1 += frame_f1(logits, y, lengths)\n",
    "        total_batches += 1\n",
    "    return total_f1 / max(total_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a17fd",
   "metadata": {},
   "source": [
    "## 7. Checkpoint & Export\n",
    "\n",
    "**EN**: Save checkpoints; export TorchScript (`.jit`) and ONNX (`.onnx`).  \n",
    "**KO**: 체크포인트 저장 및 TorchScript/ONNX 내보내기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), str(path))\n",
    "\n",
    "def export_torchscript(model, n_mels: int, path: Path, device: torch.device):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.eval()\n",
    "    x = torch.randn(1, 100, n_mels, device=device)\n",
    "    lengths = torch.tensor([100], device=device)\n",
    "    scripted = torch.jit.trace(model, (x, lengths))\n",
    "    scripted.save(str(path))\n",
    "\n",
    "def export_onnx(model, n_mels: int, path: Path, device: torch.device):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.eval()\n",
    "    x = torch.randn(1, 100, n_mels, device=device)\n",
    "    lengths = torch.tensor([100], device=device)\n",
    "    torch.onnx.export(\n",
    "        model, (x, lengths), str(path),\n",
    "        input_names=[\"x\", \"lengths\"],\n",
    "        output_names=[\"logits\"],\n",
    "        opset_version=17,\n",
    "        dynamic_axes={\"x\": {1: \"time\"}, \"logits\": {1: \"time\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3bf68",
   "metadata": {},
   "source": [
    "## 8. Data Loading Helpers\n",
    "\n",
    "**EN**: Read lists of `(wav_path, label_npy_path)` from JSON; scan noise directory; make loaders.  \n",
    "**KO**: JSON에서 `(오디오, 라벨)` 목록을 읽고, 노이즈 디렉터리 스캔 및 DataLoader 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_item_list(json_path: str) -> List[Tuple[Path, Path]]:\n",
    "    if not json_path:\n",
    "        return []\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pairs = json.load(f)\n",
    "    return [(Path(a), Path(b)) for a, b in pairs]\n",
    "\n",
    "def scan_noise(noise_dir: str) -> List[Path]:\n",
    "    if not noise_dir:\n",
    "        return []\n",
    "    p = Path(noise_dir)\n",
    "    wavs = sorted(list(p.rglob(\"*.wav\")))\n",
    "    return wavs\n",
    "\n",
    "def make_loader(items: List[Tuple[Path, Path]], args, shuffle: bool) -> DataLoader:\n",
    "    ds = FrameLabelDataset(\n",
    "        items,\n",
    "        sr=args.sr,\n",
    "        n_fft=args.n_fft,\n",
    "        hop_length=args.hop_length,\n",
    "        win_length=args.win_length,\n",
    "        n_mels=args.n_mels,\n",
    "        augment_noise_paths=scan_noise(args.noise_dir) if shuffle else [],\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=args.batch_size, shuffle=shuffle, num_workers=2, collate_fn=collate_pad, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad2554",
   "metadata": {},
   "source": [
    "## 9. Configuration & Main Entrypoint\n",
    "\n",
    "**EN**: Hyperparameters and I/O settings. Set `export=True` to produce `.jit` and `.onnx`.  \n",
    "**KO**: 하이퍼파라미터/입출력 설정. `export=True`로 내보내기(.jit/.onnx) 활성화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa87cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_default_args():\n",
    "    p = argparse.ArgumentParser(description=\"Train CRNN VAD\")\n",
    "    # Data\n",
    "    p.add_argument(\"--train_list\", type=str, required=False, default=\"\", help=\"Path to JSON list of (wav,label) for train\")\n",
    "    p.add_argument(\"--valid_list\", type=str, required=False, default=\"\", help=\"Path to JSON list of (wav,label) for valid\")\n",
    "    p.add_argument(\"--noise_dir\", type=str, default=\"\", help=\"Optional dir of noise wavs for augmentation\")\n",
    "    # Audio / Features\n",
    "    p.add_argument(\"--sr\", type=int, default=16000)\n",
    "    p.add_argument(\"--n_mels\", type=int, default=64)\n",
    "    p.add_argument(\"--n_fft\", type=int, default=1024)\n",
    "    p.add_argument(\"--hop_length\", type=int, default=160)\n",
    "    p.add_argument(\"--win_length\", type=int, default=400)\n",
    "    # Model / Train\n",
    "    p.add_argument(\"--cnn_channels\", type=int, default=64)\n",
    "    p.add_argument(\"--rnn_hidden\", type=int, default=128)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    p.add_argument(\"--epochs\", type=int, default=10)\n",
    "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    p.add_argument(\"--seed\", type=int, default=1337)\n",
    "    # I/O\n",
    "    p.add_argument(\"--out_dir\", type=str, default=\"vad_out\")\n",
    "    p.add_argument(\"--save_every\", type=int, default=1)\n",
    "    p.add_argument(\"--export\", action=\"store_true\", help=\"Export TorchScript and ONNX at the end\")\n",
    "    args, _ = p.parse_known_args([])  # in-notebook safe\n",
    "    return args\n",
    "\n",
    "def run_training(args=None):\n",
    "    if args is None:\n",
    "        args = get_default_args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[info] device={device}\")\n",
    "\n",
    "    train_items = load_item_list(args.train_list)\n",
    "    valid_items = load_item_list(args.valid_list)\n",
    "\n",
    "    if not train_items:\n",
    "        print(\"[warn] train_list is empty. Provide a JSON list of [(wav,label), ...]. See 'Data Format' section below.\")\n",
    "    if not valid_items:\n",
    "        print(\"[warn] valid_list is empty. Using train split for eval.\")\n",
    "        valid_items = train_items\n",
    "\n",
    "    model = CRNNVAD(n_mels=args.n_mels, cnn_channels=args.cnn_channels, rnn_hidden=args.rnn_hidden).to(device)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    train_loader = make_loader(train_items, args, shuffle=True) if train_items else None\n",
    "    valid_loader = make_loader(valid_items, args, shuffle=False) if valid_items else None\n",
    "\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        t0 = time.time()\n",
    "        if train_loader is not None:\n",
    "            train_loss = train_one_epoch(model, train_loader, optim, scaler, device)\n",
    "        else:\n",
    "            train_loss = float(\"nan\")\n",
    "        val_f1 = evaluate(model, valid_loader, device) if valid_loader is not None else float(\"nan\")\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[epoch {epoch:03d}] loss={train_loss:.4f} val_f1={val_f1:.4f} time={dt:.1f}s\")\n",
    "\n",
    "        if (epoch % args.save_every) == 0:\n",
    "            save_checkpoint(model, out_dir / f\"crnn_vad_epoch{epoch:03d}.pth\")\n",
    "\n",
    "        if not math.isnan(val_f1) and val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            save_checkpoint(model, out_dir / \"best.pth\")\n",
    "\n",
    "    if args.export:\n",
    "        export_torchscript(model, args.n_mels, out_dir / \"vad_crnn.jit\", device)\n",
    "        try:\n",
    "            export_onnx(model, args.n_mels, out_dir / \"vad_crnn.onnx\", device)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] ONNX export failed: {e}\")\n",
    "\n",
    "    cfg = vars(args).copy()\n",
    "    with open(out_dir / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[done] outputs saved to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7f3d6",
   "metadata": {},
   "source": [
    "## 10. Data Format (JSON)\n",
    "\n",
    "**EN**: `--train_list` and `--valid_list` expect a JSON array of pairs:  \n",
    "```json\n",
    "[\n",
    "  [\"/path/to/audio_000.wav\", \"/path/to/audio_000_labels.npy\"],\n",
    "  [\"/path/to/audio_001.wav\", \"/path/to/audio_001_labels.npy\"]\n",
    "]\n",
    "```\n",
    "The label `.npy` should be a 1D array of shape `(T,)` with values `{0,1}` aligned to log-mel frames.\n",
    "\n",
    "**KO**: `--train_list`, `--valid_list`는 다음 형식의 JSON 배열을 기대합니다.  \n",
    "라벨 `.npy`는 `(T,)` 1차원 배열이며 각 프레임의 발화 여부 `{0,1}`를 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Quick schema check helper\n",
    "def preview_list(json_path: str, max_rows: int = 3):\n",
    "    pairs = load_item_list(json_path)\n",
    "    print(f'#items={len(pairs)}')\n",
    "    for i, (a,b) in enumerate(pairs[:max_rows]):\n",
    "        print(i, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f841c4",
   "metadata": {},
   "source": [
    "## 11. Usage (In-Notebook)\n",
    "\n",
    "**EN**: Set `args.train_list` / `args.valid_list` to your JSON files and run `run_training(args)`.  \n",
    "**KO**: `args.train_list` / `args.valid_list`에 JSON 경로 설정 후 `run_training(args)` 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# args = get_default_args()\n",
    "# args.train_list = \"/content/train_list.json\"\n",
    "# args.valid_list = \"/content/valid_list.json\"\n",
    "# args.noise_dir  = \"/content/noise_wavs\"\n",
    "# args.epochs = 5\n",
    "# args.export = True\n",
    "# run_training(args)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
